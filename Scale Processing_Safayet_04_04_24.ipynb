{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0wRg5Sar3p+8POfHZ0tw9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFXWUt1_jUTx","executionInfo":{"status":"ok","timestamp":1712259281391,"user_tz":240,"elapsed":23089,"user":{"displayName":"Mohammad Safayet Khan","userId":"16149571769203166921"}},"outputId":"67538772-b020-4cd3-fbb3-7f7c26c4d4db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Assumes scale images are on your Drive under \"Images\"\n","\n","!ls /content/drive/MyDrive/Images/*csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZ1atSYcjmmv","executionInfo":{"status":"ok","timestamp":1712259296732,"user_tz":240,"elapsed":159,"user":{"displayName":"Mohammad Safayet Khan","userId":"16149571769203166921"}},"outputId":"b1b0c953-143b-4a47-a837-5f2c34b0c991"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Images/labels.csv\n","/content/drive/MyDrive/Images/SalmonFishScalesCoordinates-V2.csv\n"]}]},{"cell_type":"code","source":["import os.path\n","import numpy as np\n","import math\n","from matplotlib.image import imread\n","import matplotlib.pyplot as plt\n","import skimage.draw as drw\n","\n","SCALE_WIDTH = 1500\n","SCALE_HEIGHT = 2000\n","IMAGE_HEIGHT = 1920\n","TRANSECT_LENGTH = 750\n","\n","PATH=os.sep + os.path.join(\"content\", \"drive\", \"MyDrive\", \"Images\")\n","\n","def rgb2gray(rgb):\n","    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n","\n","def fill_boarder(gray_img, thickness, fill_value=255):\n","  gray_img[:thickness,:] = fill_value\n","  gray_img[-thickness:,:] = fill_value\n","  gray_img[:,:thickness] = fill_value\n","  gray_img[:,-thickness:] = fill_value\n","\n","def scale_edge_detector(x,index, window_size, frequency_factor, orientation):\n","  if not (orientation == 'LR' or orientation == 'RL'):\n","    raise Exception(\"Edge detection orientation must be 'LR' or 'RL'.\")\n","\n","  if window_size % 2 != 1:\n","    raise Exception(\"Window size must be an odd number.\")\n","\n","  l_sum = np.sum(x[index - int(window_size / 2): index])\n","  r_sum = np.sum(x[index + 1: index + int(window_size / 2)])\n","\n","  if orientation == 'LR':\n","    if r_sum > l_sum * frequency_factor:\n","      return True\n","    else:\n","      return False\n","  else:\n","    if r_sum * frequency_factor > l_sum:\n","      return True\n","    else:\n","      return False\n","\n","def mask1d(gray_img, kernel_size, threshold=20):\n","  if kernel_size % 2 != 1:\n","    raise Exception(\"Kernel size must be an odd number.\")\n","\n","\n","  mask = np.zeros(gray_img.shape, dtype=np.uint8)\n","  filter = np.ones(kernel_size) / kernel_size\n","\n","  for y in range(gray_img.shape[0]):\n","    delta = np.convolve(gray_img[y,:], filter, mode='same') - gray_img[y,:]\n","    mask[y, (np.abs(delta) > threshold)] = 255.\n","\n","  return mask\n","\n","def mask(gray_img, kernel_size, threshold=20):\n","  if kernel_size % 2 != 1:\n","    raise Exception(\"Kernel size must be an odd number.\")\n","\n","  xd = mask1d(gray_img, 9, 20)\n","  yd = mask1d(np.transpose(gray_img), 9, 20).transpose()\n","  mask = xd /2 + yd / 2\n","\n","  return mask\n","\n","def denoise(gray_img, kernel_size, percent_activated_threshold=0.4):\n","  if kernel_size % 2 != 1:\n","    raise Exception(\"Kernel size must be an odd number.\")\n","\n","  activation_cutoff = percent_activated_threshold * kernel_size * kernel_size * 255\n","  denoised_img = gray_img.copy()\n","\n","  for j in range(int(kernel_size / 2), gray_img.shape[0] - int(kernel_size / 2)):\n","    for k in range(int(kernel_size / 2), gray_img.shape[1] - int(kernel_size / 2)):\n","      if gray_img[j, k] > 120:\n","        if np.sum(gray_img[j:j+kernel_size,k:k+kernel_size]) < activation_cutoff:\n","          denoised_img[j, k] = 0\n","\n","  return denoised_img\n","\n","def detect_scale_contour(gray_img, padding=25):\n","  contour = []\n","\n","  for j in range(padding, gray_img.shape[0] - padding):\n","    k = padding\n","    k_min = padding\n","\n","    # Burn in period to make sure we are not detecting a scale on the edge\n","    while k < gray_img.shape[1] - padding and np.sum(gray_img[j, k-10:k+25]) > 30 * 255:\n","      k = k + 1\n","\n","    while k < gray_img.shape[1] - padding:\n","      if gray_img[j, k] > 0 and scale_edge_detector(gray_img[j,:], k, 25, 5, 'LR'):\n","        contour.append((j, k))\n","        k_min = k\n","        k = gray_img.shape[1] - padding\n","      k = k + 1\n","\n","    # Burn in period to make sure we are not detecting a scale on the edge\n","    while k > k_min and np.sum(gray_img[j, k-25:k+10]) > 30 * 255:\n","      k = k - 1\n","\n","    while k > k_min:\n","      if gray_img[j, k] > 0 and scale_edge_detector(gray_img[j,:], k, 25, 5, 'RL'):\n","        contour.append((j, k))\n","        k = 0\n","      k = k - 1\n","\n","  return contour\n","\n","def dilate_contour(contour, thickness):\n","\n","  dialated_contour = set()\n","  for (j, k) in contour:\n","    for l in range(j - thickness, j + thickness):\n","      for m in range(k - thickness, k + thickness):\n","        dialated_contour.add((l, m))\n","\n","  return list(dialated_contour)\n","\n","def draw_box(gray_img, top_left_point, bottom_right_point, color=255):\n","  j1, k1 = top_left_point\n","  j2, k2 = bottom_right_point\n","\n","  gray_img[j1,k1:k2+1] = color\n","  gray_img[j2,k1:k2+1] = color\n","  gray_img[j1:j2+1,k1] = color\n","  gray_img[j1:j2+1,k2] = color\n","\n","def draw_line(gray_img, mid_point, end_point, color=255):\n","\n","  rr, cc = drw.line(*mid_point, *end_point)\n","  gray_img[rr, cc] = color\n","\n","def draw_contour(size, contour, color=255):\n","\n","  img = np.zeros(size, dtype=np.uint8)\n","  for (j, k) in contour:\n","    img[j, k] = color\n","\n","  return img\n","\n","def load_img_to_gray(file):\n","  img = imread(file)\n","  gray = rgb2gray(img)\n","\n","  return gray\n","\n","def show_image_and_contour(gray):\n","\n","  plt.imshow(gray, cmap=plt.get_cmap('gray'))\n","  plt.show()\n","\n","  masked_image = mask(gray, 9, 20)\n","  denoised_mask = denoise(masked_image, 101, .025)\n","\n","  contour = detect_scale_contour(denoised_mask, 100)\n","  dilated_contour = dilate_contour(contour, 5)\n","  drawn_contour = draw_contour(denoised_mask.shape, dilated_contour)\n","\n","  plt.imshow(np.abs(drawn_contour - 255), cmap=plt.get_cmap('gray'))\n","  plt.show()\n","\n","def frame_image_w_mean(gray, j_m, k_m):\n","  mean_pixel_value = np.mean(gray)\n","\n","  if j_m - SCALE_HEIGHT // 2 > 0:\n","    gray[0:j_m - SCALE_HEIGHT // 2,:] = mean_pixel_value\n","    gray[j_m + SCALE_HEIGHT // 2:,:] = mean_pixel_value\n","\n","  if k_m - SCALE_WIDTH // 2 > 0:\n","    gray[:, 0:k_m - SCALE_WIDTH // 2] = mean_pixel_value\n","    gray[:,k_m + SCALE_WIDTH // 2:] = mean_pixel_value\n","\n","  #return gray\n","\n","def calculate_transect_endpoint(mid_point, offset, length):\n","  \"\"\"\n","  Given an offset from a line extending direclty down from the\n","  mid-point, determine the coordinates of the other endpoint of\n","  a line that starts at the mid-point and extends in the direction\n","  of the offset of specified length\n","\n","  :param mid_point: the coordinates of the mid-point in row-major format\n","  :param offset: offset angle in radians from 6 o'clock applied counter-clockwise\n","  :param length: describe about parameter p3\n","  :return: the coordinates of the endpoint in row-major format\n","  \"\"\"\n","  j, k = mid_point\n","\n","  return (j + int(length * math.cos(offset)),\n","          k + int(length * math.sin(offset)))\n","\n","def calculate_transect_angle(mid_point, end_point):\n","  \"\"\"\n","  Given two points, calculated the angle between a line that\n","  extends down from the mid-point, opens counter-clockwise\n","  to the line that is defined by the mid_point and end_point\n","\n","  :param mid_point: the coordinates of the mid-point in row-major format\n","  :param end_point: the coordinates of the end-point of the transect\n","     in row-major format\n","  :return: angle in radians\n","  \"\"\"\n","\n","  offset = 0\n","\n","  # if the transect angle is at pi/2 or 3 pi/2, the atan will be undefined\n","  if abs(mid_point[0] - end_point[0]) < 1e-5:\n","    if mid_point[0] < end_point[0]:\n","      offset = math.pi / 2\n","    else:\n","      offset = 3 * math.pi / 2\n","\n","  else:\n","    offset = math.atan((mid_point[1] - end_point[1]) /\n","                       (mid_point[0] - end_point[0]))\n","\n","    # if end point is above mid point, add pi to offset as it would have\n","    #  been caculated relative to line extending above mid point.\n","    if(end_point[0] < mid_point[0]):\n","      offset = offset + math.pi\n","\n","    # if offset is negative, add to 2pi\n","    if(end_point[1] < mid_point[1]):\n","      offset = 2 * math.pi + offset\n","\n","  return offset\n","\n","def calculate_coordinates_of_transects(mid_point, transect_angle, offset_step_size, transect_count):\n","  \"\"\"\n","  Determine the coordintates of points along transect lines at specified offsets\n","  from an initial trasect.\n","\n","  :param mid_point: the coordinates of the mid-point in row-major format\n","  :param offset: transect angle in radians from 6 o'clock applied counter-clockwise\n","  :param offset_step_size: Offset in radians between transects\n","  :param transect_count: The number of transects to generate in addition to\n","    main transect\n","  :return: a list of lists containing the coordinates of points on transects\n","    extending out from the mid_point.  Indexing of transects alternates out\n","    from main transect.\n","  \"\"\"\n","  transects_coordinates = []\n","  calculate_transect_endpoint(mid_point, transect_angle, TRANSECT_LENGTH)\n","  transects_coordinates.append(drw.line(*mid_point,\n","                                        *calculate_transect_endpoint(mid_point, transect_angle, TRANSECT_LENGTH))\n","                               )\n","\n","  for offset in [o for i in range(1, int(transect_count / 2) + 1) for o in (i * offset_step_size, -i * offset_step_size)]:\n","    transects_coordinates.append(drw.line(*mid_point,\n","                                          *calculate_transect_endpoint(mid_point, transect_angle + offset, TRANSECT_LENGTH))\n","                                 )\n","\n","  return transects_coordinates\n","\n","def draw_transects(gray_img, transects_coordinates, color = 250):\n","  \"\"\"\n","  Draws transects on a copy of grayscale image.\n","\n","  :param gray_img: 2d array of a grayscale image\n","  :param trasects_coordinates: a list of lists containing the coordinates of points on transects\n","    extending out from the mid_point.  Indexing of transects alternates out\n","    from main transect.\n","  :param color: pixel value for drawn transect\n","  :return: 2d array representing grayscale image with transects applied\n","  \"\"\"\n","  img_w_transects = np.copy(gray_img)\n","  j_max = gray_img.shape[0]\n","  k_max = gray_img.shape[1]\n","\n","  for i, transect in enumerate(transects_coordinates):\n","    rr, cc = transect\n","    rr[rr >= j_max] = j_max - 1\n","    cc[cc >= k_max] = k_max - 1\n","\n","    img_w_transects[rr, cc] = color\n","\n","  return img_w_transects\n","\n","def extract_transects_signals(gray_img, transects_coordinates):\n","\n","  transect_signals = []\n","  for i, transect in enumerate(transects_coordinates):\n","    signal = []\n","    rr, cc = transect\n","    for j, k in zip(rr, cc):\n","      signal.append(gray_img[j, k])\n","    transect_signals.append(signal)\n","\n","  return transect_signals\n","\n","def convolve_transects_signals(transects_signals, kernel_size):\n","\n","  filter = np.ones(kernel_size) / kernel_size\n","  convolved_signals = []\n","\n","  for signal in transects_signals:\n","    convolved_signals.append(convolve_transects_signal(signal, kernel_size))\n","\n","  return convolved_signals\n","\n","def convolve_transects_signal(transects_signal, kernel_size):\n","\n","  filter = np.ones(kernel_size) / kernel_size\n","\n","  return(np.convolve(transects_signal, filter, mode='same') - transects_signal)\n","\n","\n","def plot_transect_signals(transects_signals, figure_path, figure_filename):\n","  n = len(transects)\n","\n","  fig, axes = plt.subplots(2 * n)\n","  for i, signal in enumerate(transects_signals):\n","    axes[i].plot(signal)\n","\n","  convolved_signals = convolve_transects_signals(transects_signals, 5)\n","  for i, signal in enumerate(convolved_signals):\n","    axes[i + n].plot(signal)\n","\n","  fig.savefig(os.path.join(figure_path, figure_filename +\".png\"), format=\"png\",\n","              bbox_inches=\"tight\")\n","\n","def plot_transect_signals_for_image(filepath, mid_point, transect_angle,\n","                                    offset_step_size, transect_count,\n","                                    figure_path):\n","  gray = load_img_to_gray(filepath)\n","  filename = os.path.splitext(os.path.basename(filepath))[0]\n","\n","  transects = calculate_coordinates_of_transects(mid_point, transect_angle,\n","                                                 offset_step_size, transect_count)\n","  transects_signals = extract_transects_signals(gray, transects)\n","\n","  plot_transect_signals(transects_signals, figure_path, filename)"],"metadata":{"id":"fUuN0YmbjpmG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from ast import literal_eval\n","\n","scale_data = pd.read_csv('/content/drive/MyDrive/Images/SalmonFishScalesCoordinates-V2.csv', sep=\",\",\n","                          names=['filename', 'lt_center', 'br_center', 'end_point'],\n","                          dtype={'filename': 'string'},\n","                          converters={'lt_center': literal_eval,\n","                                      'br_center': literal_eval,\n","                                      'end_point': literal_eval},\n","                          header=0)\n","scale_data['filepath'] = scale_data.apply(lambda row: os.path.join(PATH, row['filename']) +  \".tif\", axis=1)\n","\n","# Switch from column-major to row major\n","scale_data['lt_center'] = scale_data.apply(lambda row: (row['lt_center'][1], row['lt_center'][0]) , axis = 1)\n","scale_data['br_center'] = scale_data.apply(lambda row: (row['br_center'][1], row['br_center'][0]) , axis = 1)\n","scale_data['end_point'] = scale_data.apply(lambda row: (row['end_point'][1], row['end_point'][0]) , axis = 1)\n","\n","scale_data['mid_center'] = scale_data.apply(lambda row: (int((row['lt_center'][0] + row['br_center'][0]) / 2),\n","                                                         int((row['lt_center'][1] + row['br_center'][1]) / 2)) , axis=1)\n","scale_data['transect_angle'] = scale_data.apply(lambda row: calculate_transect_angle(row['mid_center'], row['end_point']), axis=1)\n","\n","scale_data['id'] = scale_data.apply(lambda row: int(row['filename'].split('_')[0]), axis=1)\n","\n","# Bring in labels\n","label_data = pd.read_csv('/content/drive/MyDrive/Images/labels.csv', sep=\",\",\n","                          names=['id', 'age', 'type', 'fold'],\n","                          dtype={'filename': 'int64', 'age': 'int32', 'type': 'int32', 'fold': 'int32'},\n","                          header=0)\n","\n","# Fold in labels with scale data\n","scale_data = scale_data.merge(label_data, on='id', how='left', validate='many_to_one')\n","\n","# Drop any rows with NaNs, recast ints\n","scale_data = scale_data.dropna().astype({'fold': int, 'type': int})"],"metadata":{"id":"LGnVSiy1kS3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get transect signals\n","transects_signals = []\n","\n","for index, row in scale_data.iterrows():\n","  #print(index, row['id'], row['filename'], row['fold'])\n","\n","  gray = load_img_to_gray(row['filepath'])\n","  transects = calculate_coordinates_of_transects(row['mid_center'],\n","                                                 row['transect_angle'],\n","                                                 .1, 5)\n","\n","  transects_signals.append([row['id'],\n","                           extract_transects_signals(gray, transects)],\n","                           )\n","\n","transects_signals_df = pd.DataFrame(transects_signals, columns=['id', 'signals'])\n","\n","# Merge with labels and explode\n","transects_signals_df = transects_signals_df.merge(label_data, on='id',\n","                                                  how='left',\n","                                                  validate='many_to_one')\\\n","                                                  .explode('signals', ignore_index=True)\\\n","                                                  .rename(columns={\"signals\": \"signal\"})\n","transects_signals_df['signal'] = transects_signals_df['signal'].apply(lambda x: np.array(x))\n","\n","# Add convolved signals\n","transects_signals_df['convolved_signal'] = transects_signals_df.apply(lambda row: convolve_transects_signal(row['signal'], 5), axis=1)\n","\n"],"metadata":{"id":"M7BfT-mSkT5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transects_signals_df contains the intensities of transects, type, age, and fold\n","# you should be able to use these to train models\n","\n","# Create a copy of transects_signals_df so that it doesn't have to be generated\n","#  each time\n","\n","def rolling_window(a, window, step):\n","    shape = a.shape[:-1] + ((a.shape[-1] - window + 1)//step, window)\n","    strides = (a.strides[0] * step,) + (a.strides[-1],)\n","    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n","\n","#print(rolling_window(s[2], 25, 1).shape)\n","\n","df = transects_signals_df.copy(deep=True)\n","df['signal'] = df['signal'].apply(lambda x: np.pad(x, (0, 2 * TRANSECT_LENGTH - x.size), 'constant', constant_values=(0, 0)))\n","df['convolved_signal'] = df['convolved_signal'].apply(lambda x: np.pad(x, (0, 2 * TRANSECT_LENGTH - x.size), 'constant', constant_values=(0, 0)))\n","\n","import keras\n","\n","# Untuned example from\n","# https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n","\n","def make_model(input_shape):\n","    input_layer = keras.layers.Input(input_shape)\n","\n","    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n","    conv1 = keras.layers.BatchNormalization()(conv1)\n","    conv1 = keras.layers.ReLU()(conv1)\n","\n","    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n","    conv2 = keras.layers.BatchNormalization()(conv2)\n","    conv2 = keras.layers.ReLU()(conv2)\n","\n","    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n","    conv3 = keras.layers.BatchNormalization()(conv3)\n","    conv3 = keras.layers.ReLU()(conv3)\n","\n","    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n","\n","    output_layer = keras.layers.Dense(2, activation=\"softmax\")(gap)\n","\n","    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n","\n","train = df[df['fold'] < 9][['signal', 'type']].to_numpy(copy=True)\n","val = df[df['fold'] >= 9][['signal', 'type']].to_numpy(copy=True)\n","\n","print(train.shape)\n","\n","x_train = np.stack(train[:,0])\n","y_train = train[:,1].astype(int)\n","\n","x_val = np.stack(val[:,0])\n","y_val = val[:,1].astype(int)\n","\n","x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n","x_val = x_val.reshape((x_val.shape[0], x_val.shape[1], 1))\n","\n","model = make_model(input_shape=x_train.shape[1:])\n","\n","epochs = 500\n","batch_size = 32\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\n","        \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"\n","    ),\n","    keras.callbacks.ReduceLROnPlateau(\n","        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n","    ),\n","    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n","]\n","model.compile(\n","    optimizer=\"adam\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"sparse_categorical_accuracy\"],\n",")\n","\n","history = model.fit(\n","    x_train,\n","    y_train,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    callbacks=callbacks,\n","    validation_split=0.2,\n","    verbose=1,\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jz8CPjq_lAWi","executionInfo":{"status":"ok","timestamp":1712260155123,"user_tz":240,"elapsed":676169,"user":{"displayName":"Mohammad Safayet Khan","userId":"16149571769203166921"}},"outputId":"e0663cea-8f3b-4ee9-e446-ed0eac4c851d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(895, 2)\n","Epoch 1/500\n","23/23 [==============================] - 10s 332ms/step - loss: 0.6416 - sparse_categorical_accuracy: 0.6257 - val_loss: 2.1577 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 2/500\n","23/23 [==============================] - 8s 355ms/step - loss: 0.6210 - sparse_categorical_accuracy: 0.6439 - val_loss: 2.7292 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 3/500\n","23/23 [==============================] - 8s 347ms/step - loss: 0.6277 - sparse_categorical_accuracy: 0.6383 - val_loss: 2.5375 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 4/500\n","23/23 [==============================] - 7s 301ms/step - loss: 0.6274 - sparse_categorical_accuracy: 0.6425 - val_loss: 2.2031 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 5/500\n","23/23 [==============================] - 8s 373ms/step - loss: 0.6183 - sparse_categorical_accuracy: 0.6355 - val_loss: 1.6337 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 6/500\n","23/23 [==============================] - 7s 301ms/step - loss: 0.6167 - sparse_categorical_accuracy: 0.6494 - val_loss: 1.8724 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 7/500\n","23/23 [==============================] - 8s 370ms/step - loss: 0.6151 - sparse_categorical_accuracy: 0.6439 - val_loss: 1.2900 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 8/500\n","23/23 [==============================] - 7s 288ms/step - loss: 0.6134 - sparse_categorical_accuracy: 0.6299 - val_loss: 1.3908 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 9/500\n","23/23 [==============================] - 8s 351ms/step - loss: 0.6216 - sparse_categorical_accuracy: 0.6522 - val_loss: 1.1729 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 10/500\n","23/23 [==============================] - 7s 293ms/step - loss: 0.6179 - sparse_categorical_accuracy: 0.6439 - val_loss: 0.8156 - val_sparse_categorical_accuracy: 0.4246 - lr: 0.0010\n","Epoch 11/500\n","23/23 [==============================] - 8s 351ms/step - loss: 0.6066 - sparse_categorical_accuracy: 0.6480 - val_loss: 0.7483 - val_sparse_categorical_accuracy: 0.5084 - lr: 0.0010\n","Epoch 12/500\n","23/23 [==============================] - 7s 307ms/step - loss: 0.6160 - sparse_categorical_accuracy: 0.6341 - val_loss: 0.7263 - val_sparse_categorical_accuracy: 0.4525 - lr: 0.0010\n","Epoch 13/500\n","23/23 [==============================] - 8s 335ms/step - loss: 0.6190 - sparse_categorical_accuracy: 0.6466 - val_loss: 1.4383 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 14/500\n","23/23 [==============================] - 7s 322ms/step - loss: 0.6143 - sparse_categorical_accuracy: 0.6564 - val_loss: 1.5380 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 15/500\n","23/23 [==============================] - 7s 317ms/step - loss: 0.6066 - sparse_categorical_accuracy: 0.6522 - val_loss: 1.2292 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 16/500\n","23/23 [==============================] - 8s 337ms/step - loss: 0.6091 - sparse_categorical_accuracy: 0.6397 - val_loss: 1.1710 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 17/500\n","23/23 [==============================] - 7s 284ms/step - loss: 0.6117 - sparse_categorical_accuracy: 0.6411 - val_loss: 0.9241 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 18/500\n","23/23 [==============================] - 8s 346ms/step - loss: 0.6081 - sparse_categorical_accuracy: 0.6536 - val_loss: 0.7372 - val_sparse_categorical_accuracy: 0.3966 - lr: 0.0010\n","Epoch 19/500\n","23/23 [==============================] - 7s 295ms/step - loss: 0.6085 - sparse_categorical_accuracy: 0.6453 - val_loss: 0.8773 - val_sparse_categorical_accuracy: 0.4134 - lr: 0.0010\n","Epoch 20/500\n","23/23 [==============================] - 8s 355ms/step - loss: 0.6153 - sparse_categorical_accuracy: 0.6494 - val_loss: 0.8230 - val_sparse_categorical_accuracy: 0.4525 - lr: 0.0010\n","Epoch 21/500\n","23/23 [==============================] - 7s 288ms/step - loss: 0.6117 - sparse_categorical_accuracy: 0.6564 - val_loss: 0.9104 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 22/500\n","23/23 [==============================] - 8s 345ms/step - loss: 0.6030 - sparse_categorical_accuracy: 0.6536 - val_loss: 0.8724 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 23/500\n","23/23 [==============================] - 6s 281ms/step - loss: 0.6055 - sparse_categorical_accuracy: 0.6494 - val_loss: 0.7811 - val_sparse_categorical_accuracy: 0.4302 - lr: 0.0010\n","Epoch 24/500\n","23/23 [==============================] - 8s 346ms/step - loss: 0.6080 - sparse_categorical_accuracy: 0.6550 - val_loss: 0.7218 - val_sparse_categorical_accuracy: 0.4134 - lr: 0.0010\n","Epoch 25/500\n","23/23 [==============================] - 7s 305ms/step - loss: 0.6126 - sparse_categorical_accuracy: 0.6536 - val_loss: 0.7249 - val_sparse_categorical_accuracy: 0.5698 - lr: 0.0010\n","Epoch 26/500\n","23/23 [==============================] - 8s 351ms/step - loss: 0.6150 - sparse_categorical_accuracy: 0.6466 - val_loss: 1.0047 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 27/500\n","23/23 [==============================] - 7s 298ms/step - loss: 0.6144 - sparse_categorical_accuracy: 0.6369 - val_loss: 1.3714 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 28/500\n","23/23 [==============================] - 9s 375ms/step - loss: 0.6081 - sparse_categorical_accuracy: 0.6425 - val_loss: 1.0288 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 29/500\n","23/23 [==============================] - 7s 298ms/step - loss: 0.6070 - sparse_categorical_accuracy: 0.6564 - val_loss: 0.8463 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 30/500\n","23/23 [==============================] - 8s 324ms/step - loss: 0.6054 - sparse_categorical_accuracy: 0.6648 - val_loss: 0.7353 - val_sparse_categorical_accuracy: 0.4190 - lr: 0.0010\n","Epoch 31/500\n","23/23 [==============================] - 7s 291ms/step - loss: 0.6002 - sparse_categorical_accuracy: 0.6774 - val_loss: 0.7286 - val_sparse_categorical_accuracy: 0.5307 - lr: 0.0010\n","Epoch 32/500\n","23/23 [==============================] - 7s 313ms/step - loss: 0.6047 - sparse_categorical_accuracy: 0.6592 - val_loss: 0.7545 - val_sparse_categorical_accuracy: 0.4525 - lr: 0.0010\n","Epoch 33/500\n","23/23 [==============================] - 7s 296ms/step - loss: 0.6112 - sparse_categorical_accuracy: 0.6564 - val_loss: 0.7315 - val_sparse_categorical_accuracy: 0.5866 - lr: 0.0010\n","Epoch 34/500\n","23/23 [==============================] - 8s 338ms/step - loss: 0.6052 - sparse_categorical_accuracy: 0.6578 - val_loss: 0.7426 - val_sparse_categorical_accuracy: 0.5587 - lr: 0.0010\n","Epoch 35/500\n","23/23 [==============================] - 7s 326ms/step - loss: 0.5975 - sparse_categorical_accuracy: 0.6550 - val_loss: 0.7434 - val_sparse_categorical_accuracy: 0.4246 - lr: 0.0010\n","Epoch 36/500\n","23/23 [==============================] - 7s 304ms/step - loss: 0.5992 - sparse_categorical_accuracy: 0.6690 - val_loss: 0.7733 - val_sparse_categorical_accuracy: 0.3855 - lr: 0.0010\n","Epoch 37/500\n","23/23 [==============================] - 11s 488ms/step - loss: 0.6022 - sparse_categorical_accuracy: 0.6718 - val_loss: 0.7280 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 38/500\n","23/23 [==============================] - 7s 321ms/step - loss: 0.5999 - sparse_categorical_accuracy: 0.6648 - val_loss: 0.7024 - val_sparse_categorical_accuracy: 0.5866 - lr: 0.0010\n","Epoch 39/500\n","23/23 [==============================] - 9s 380ms/step - loss: 0.5998 - sparse_categorical_accuracy: 0.6662 - val_loss: 0.7801 - val_sparse_categorical_accuracy: 0.4469 - lr: 0.0010\n","Epoch 40/500\n","23/23 [==============================] - 7s 312ms/step - loss: 0.6048 - sparse_categorical_accuracy: 0.6620 - val_loss: 0.8656 - val_sparse_categorical_accuracy: 0.5810 - lr: 0.0010\n","Epoch 41/500\n","23/23 [==============================] - 8s 329ms/step - loss: 0.5949 - sparse_categorical_accuracy: 0.6830 - val_loss: 0.8459 - val_sparse_categorical_accuracy: 0.6145 - lr: 0.0010\n","Epoch 42/500\n","23/23 [==============================] - 8s 348ms/step - loss: 0.6007 - sparse_categorical_accuracy: 0.6816 - val_loss: 0.7388 - val_sparse_categorical_accuracy: 0.5587 - lr: 0.0010\n","Epoch 43/500\n","23/23 [==============================] - 7s 301ms/step - loss: 0.5993 - sparse_categorical_accuracy: 0.6746 - val_loss: 0.8195 - val_sparse_categorical_accuracy: 0.4525 - lr: 0.0010\n","Epoch 44/500\n","23/23 [==============================] - 8s 359ms/step - loss: 0.6051 - sparse_categorical_accuracy: 0.6704 - val_loss: 0.7958 - val_sparse_categorical_accuracy: 0.5810 - lr: 0.0010\n","Epoch 45/500\n","23/23 [==============================] - 7s 304ms/step - loss: 0.6052 - sparse_categorical_accuracy: 0.6704 - val_loss: 0.8970 - val_sparse_categorical_accuracy: 0.5866 - lr: 0.0010\n","Epoch 46/500\n","23/23 [==============================] - 9s 380ms/step - loss: 0.6020 - sparse_categorical_accuracy: 0.6606 - val_loss: 0.8660 - val_sparse_categorical_accuracy: 0.3631 - lr: 0.0010\n","Epoch 47/500\n","23/23 [==============================] - 7s 296ms/step - loss: 0.6137 - sparse_categorical_accuracy: 0.6606 - val_loss: 0.7621 - val_sparse_categorical_accuracy: 0.3799 - lr: 0.0010\n","Epoch 48/500\n","23/23 [==============================] - 9s 378ms/step - loss: 0.6052 - sparse_categorical_accuracy: 0.6494 - val_loss: 0.9886 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 49/500\n","23/23 [==============================] - 7s 287ms/step - loss: 0.6018 - sparse_categorical_accuracy: 0.6466 - val_loss: 0.7719 - val_sparse_categorical_accuracy: 0.5978 - lr: 0.0010\n","Epoch 50/500\n","23/23 [==============================] - 8s 359ms/step - loss: 0.5958 - sparse_categorical_accuracy: 0.6774 - val_loss: 0.7635 - val_sparse_categorical_accuracy: 0.4302 - lr: 0.0010\n","Epoch 51/500\n","23/23 [==============================] - 7s 301ms/step - loss: 0.6058 - sparse_categorical_accuracy: 0.6578 - val_loss: 0.7709 - val_sparse_categorical_accuracy: 0.5866 - lr: 0.0010\n","Epoch 52/500\n","23/23 [==============================] - 8s 348ms/step - loss: 0.6013 - sparse_categorical_accuracy: 0.6718 - val_loss: 0.7315 - val_sparse_categorical_accuracy: 0.5587 - lr: 0.0010\n","Epoch 53/500\n","23/23 [==============================] - 7s 323ms/step - loss: 0.6002 - sparse_categorical_accuracy: 0.6494 - val_loss: 0.8026 - val_sparse_categorical_accuracy: 0.5922 - lr: 0.0010\n","Epoch 54/500\n","23/23 [==============================] - 7s 313ms/step - loss: 0.6026 - sparse_categorical_accuracy: 0.6578 - val_loss: 0.7690 - val_sparse_categorical_accuracy: 0.4302 - lr: 0.0010\n","Epoch 55/500\n","23/23 [==============================] - 8s 351ms/step - loss: 0.6053 - sparse_categorical_accuracy: 0.6550 - val_loss: 0.8335 - val_sparse_categorical_accuracy: 0.4637 - lr: 0.0010\n","Epoch 56/500\n","23/23 [==============================] - 7s 298ms/step - loss: 0.6039 - sparse_categorical_accuracy: 0.6522 - val_loss: 0.7431 - val_sparse_categorical_accuracy: 0.4134 - lr: 0.0010\n","Epoch 57/500\n","23/23 [==============================] - 9s 376ms/step - loss: 0.6051 - sparse_categorical_accuracy: 0.6774 - val_loss: 1.2850 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 58/500\n","23/23 [==============================] - 7s 309ms/step - loss: 0.6000 - sparse_categorical_accuracy: 0.6690 - val_loss: 0.9099 - val_sparse_categorical_accuracy: 0.4749 - lr: 0.0010\n","Epoch 59/500\n","23/23 [==============================] - 8s 350ms/step - loss: 0.5974 - sparse_categorical_accuracy: 0.6718 - val_loss: 0.8156 - val_sparse_categorical_accuracy: 0.4413 - lr: 5.0000e-04\n","Epoch 60/500\n","23/23 [==============================] - 6s 274ms/step - loss: 0.6022 - sparse_categorical_accuracy: 0.6746 - val_loss: 0.8214 - val_sparse_categorical_accuracy: 0.4581 - lr: 5.0000e-04\n","Epoch 61/500\n","23/23 [==============================] - 8s 356ms/step - loss: 0.5938 - sparse_categorical_accuracy: 0.6774 - val_loss: 0.7616 - val_sparse_categorical_accuracy: 0.5754 - lr: 5.0000e-04\n","Epoch 62/500\n","23/23 [==============================] - 7s 302ms/step - loss: 0.5884 - sparse_categorical_accuracy: 0.6858 - val_loss: 0.7400 - val_sparse_categorical_accuracy: 0.4078 - lr: 5.0000e-04\n","Epoch 63/500\n","23/23 [==============================] - 8s 352ms/step - loss: 0.5787 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.7242 - val_sparse_categorical_accuracy: 0.4469 - lr: 5.0000e-04\n","Epoch 64/500\n","23/23 [==============================] - 7s 290ms/step - loss: 0.5926 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.8488 - val_sparse_categorical_accuracy: 0.6034 - lr: 5.0000e-04\n","Epoch 65/500\n","23/23 [==============================] - 8s 345ms/step - loss: 0.5999 - sparse_categorical_accuracy: 0.6802 - val_loss: 0.8541 - val_sparse_categorical_accuracy: 0.6257 - lr: 5.0000e-04\n","Epoch 66/500\n","23/23 [==============================] - 7s 305ms/step - loss: 0.5900 - sparse_categorical_accuracy: 0.6774 - val_loss: 0.7338 - val_sparse_categorical_accuracy: 0.4469 - lr: 5.0000e-04\n","Epoch 67/500\n","23/23 [==============================] - 8s 327ms/step - loss: 0.5888 - sparse_categorical_accuracy: 0.6802 - val_loss: 0.7281 - val_sparse_categorical_accuracy: 0.4916 - lr: 5.0000e-04\n","Epoch 68/500\n","23/23 [==============================] - 7s 322ms/step - loss: 0.5862 - sparse_categorical_accuracy: 0.6927 - val_loss: 0.7821 - val_sparse_categorical_accuracy: 0.5922 - lr: 5.0000e-04\n","Epoch 69/500\n","23/23 [==============================] - 7s 310ms/step - loss: 0.5824 - sparse_categorical_accuracy: 0.7025 - val_loss: 0.7920 - val_sparse_categorical_accuracy: 0.5922 - lr: 5.0000e-04\n","Epoch 70/500\n","23/23 [==============================] - 8s 334ms/step - loss: 0.5849 - sparse_categorical_accuracy: 0.7067 - val_loss: 0.8925 - val_sparse_categorical_accuracy: 0.5642 - lr: 5.0000e-04\n","Epoch 71/500\n","23/23 [==============================] - 7s 312ms/step - loss: 0.5889 - sparse_categorical_accuracy: 0.6983 - val_loss: 0.7687 - val_sparse_categorical_accuracy: 0.5084 - lr: 5.0000e-04\n","Epoch 72/500\n","23/23 [==============================] - 9s 396ms/step - loss: 0.5891 - sparse_categorical_accuracy: 0.6802 - val_loss: 0.7651 - val_sparse_categorical_accuracy: 0.4413 - lr: 5.0000e-04\n","Epoch 73/500\n","23/23 [==============================] - 6s 278ms/step - loss: 0.5897 - sparse_categorical_accuracy: 0.6997 - val_loss: 0.8668 - val_sparse_categorical_accuracy: 0.3743 - lr: 5.0000e-04\n","Epoch 74/500\n","23/23 [==============================] - 9s 379ms/step - loss: 0.5904 - sparse_categorical_accuracy: 0.6830 - val_loss: 0.7861 - val_sparse_categorical_accuracy: 0.4302 - lr: 5.0000e-04\n","Epoch 75/500\n","23/23 [==============================] - 7s 289ms/step - loss: 0.5852 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.7931 - val_sparse_categorical_accuracy: 0.4246 - lr: 5.0000e-04\n","Epoch 76/500\n","23/23 [==============================] - 8s 360ms/step - loss: 0.5922 - sparse_categorical_accuracy: 0.6941 - val_loss: 0.7798 - val_sparse_categorical_accuracy: 0.5196 - lr: 5.0000e-04\n","Epoch 77/500\n","23/23 [==============================] - 7s 317ms/step - loss: 0.5908 - sparse_categorical_accuracy: 0.6969 - val_loss: 1.5693 - val_sparse_categorical_accuracy: 0.5754 - lr: 5.0000e-04\n","Epoch 78/500\n","23/23 [==============================] - 8s 363ms/step - loss: 0.5959 - sparse_categorical_accuracy: 0.6760 - val_loss: 0.7983 - val_sparse_categorical_accuracy: 0.5307 - lr: 5.0000e-04\n","Epoch 79/500\n","23/23 [==============================] - 6s 278ms/step - loss: 0.5782 - sparse_categorical_accuracy: 0.7011 - val_loss: 0.7868 - val_sparse_categorical_accuracy: 0.5419 - lr: 2.5000e-04\n","Epoch 80/500\n","23/23 [==============================] - 10s 427ms/step - loss: 0.5812 - sparse_categorical_accuracy: 0.6899 - val_loss: 0.7605 - val_sparse_categorical_accuracy: 0.4469 - lr: 2.5000e-04\n","Epoch 81/500\n","23/23 [==============================] - 10s 420ms/step - loss: 0.5755 - sparse_categorical_accuracy: 0.6899 - val_loss: 0.7630 - val_sparse_categorical_accuracy: 0.4134 - lr: 2.5000e-04\n","Epoch 82/500\n","23/23 [==============================] - 6s 282ms/step - loss: 0.5774 - sparse_categorical_accuracy: 0.6774 - val_loss: 0.7634 - val_sparse_categorical_accuracy: 0.4693 - lr: 2.5000e-04\n","Epoch 83/500\n","23/23 [==============================] - 8s 366ms/step - loss: 0.5837 - sparse_categorical_accuracy: 0.6913 - val_loss: 0.7704 - val_sparse_categorical_accuracy: 0.5140 - lr: 2.5000e-04\n","Epoch 84/500\n","23/23 [==============================] - 7s 305ms/step - loss: 0.5828 - sparse_categorical_accuracy: 0.7011 - val_loss: 0.7900 - val_sparse_categorical_accuracy: 0.5307 - lr: 2.5000e-04\n","Epoch 85/500\n","23/23 [==============================] - 8s 359ms/step - loss: 0.5747 - sparse_categorical_accuracy: 0.7053 - val_loss: 0.8367 - val_sparse_categorical_accuracy: 0.6145 - lr: 2.5000e-04\n","Epoch 86/500\n","23/23 [==============================] - 7s 290ms/step - loss: 0.5786 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.7608 - val_sparse_categorical_accuracy: 0.4469 - lr: 2.5000e-04\n","Epoch 87/500\n","23/23 [==============================] - 9s 381ms/step - loss: 0.5730 - sparse_categorical_accuracy: 0.6997 - val_loss: 0.9045 - val_sparse_categorical_accuracy: 0.6089 - lr: 2.5000e-04\n","Epoch 88/500\n","23/23 [==============================] - 7s 299ms/step - loss: 0.5787 - sparse_categorical_accuracy: 0.7095 - val_loss: 0.7714 - val_sparse_categorical_accuracy: 0.4358 - lr: 2.5000e-04\n","Epoch 88: early stopping\n"]}]}]}